{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8184733a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing OCR...\n",
      "OCR Result:\n",
      "21\n",
      "Introdução\n",
      "\n",
      "Na maioria das aplicações reais de classificação, previsão, aproximação e\n",
      "otimização, as bases de dados contêm um grande número de caraterísticas, muitas\n",
      "delas introduzidas para obter uma melhor representação do problema, tais como,\n",
      "por exemplo, nome, identidade, endereço, etc. Entretanto, na maioria dos casos,\n",
      "grande parte destas caraterísticas são irrelevantes e/ou redundantes. Deste modo,\n",
      "\n",
      "um problema comum nestas aplicações reais é a seleção das características.\n",
      "\n",
      "AA seleção de características se refere a um processo no qual um espaço de\n",
      "dados é transformado em um espaço de características, de menor dimensão, mas\n",
      "que ainda retenha a maior parte da  informação intrínseca dos dados; em outras\n",
      "palavras, o conjunto de dados sofre uma redução de dimensionalidade. Os\n",
      "métodos de seleção de características tratam exatamente da escolha, dentre todos\n",
      "os atributos da base de dados, daqueles mais relevantes do ponto de vista da\n",
      "\n",
      "informação [MARD79], [DASH97].\n",
      "\n",
      "\n",
      "Performing Summarization...\n",
      "Summary:\n",
      "Os\n",
      "métodos de seleção de características tratam exatamente da escolha, dentre todos\n",
      "os atributos da base de dados, daqueles mais relevantes do ponto de vista da\n",
      "\n",
      "informação [MARD79], [DASH97]. Entretanto, na maioria dos casos,\n",
      "grande parte destas caraterísticas são irrelevantes e/ou redundantes. Deste modo,\n",
      "\n",
      "um problema comum nestas aplicações reais é a seleção das características.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "import spacy\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "# Carregar modelo de linguagem do SpaCy\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "# Baixar as stopwords do NLTK\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Função para realizar OCR em uma imagem\n",
    "def perform_ocr(image_path):\n",
    "    # Abrir a imagem\n",
    "    image = Image.open(image_path)\n",
    "    # Realizar OCR usando Tesseract\n",
    "    text = pytesseract.image_to_string(image, lang='por')\n",
    "    return text\n",
    "\n",
    "# Função para realizar a sumarização de texto\n",
    "def perform_summarization(text):\n",
    "    # Tokenize o texto em frases, divide o texto em frases usando a função sent_tokenize da biblioteca NLTK, considerando o idioma ingles padrao.\n",
    "    sentences = sent_tokenize(text)\n",
    "    # Tokenize as palavras, divide o texto em palavras usando a função word_tokenize da biblioteca NLTK.\n",
    "    words = word_tokenize(text)\n",
    "    # Remova stopwords, remove as palavras irrelevantes (stop words) do texto.\n",
    "    stop_words = set(stopwords.words(\"portuguese\"))\n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "    # Calcule a frequência das palavras, calcula a frequência das palavras no texto usando a classe FreqDist da biblioteca NLTK.\n",
    "    freq_dist = FreqDist(words)\n",
    "    # Calcule a frequência máxima, calcula a frequência máxima entre todas as palavras no texto.\n",
    "    max_freq = max(freq_dist.values())\n",
    "    # Normalizar as frequências, normaliza as frequências das palavras dividindo a frequência de cada palavra pela frequência máxima.\n",
    "    for word in freq_dist.keys():\n",
    "        freq_dist[word] = (freq_dist[word]/max_freq)\n",
    "    # Calcule a pontuação das frases\n",
    "    sentence_scores = {}\n",
    "    for sentence in sentences:\n",
    "        #Divide a frase em palavras, converte para minúsculas\n",
    "        for word in word_tokenize(sentence.lower(), language='portuguese'):  \n",
    "            #Verifica se a palavra está presente nas palavras relevantes (que não são stop words).\n",
    "            if word in freq_dist.keys():\n",
    "                #Verifica se a frase tem menos de 30 palavras.\n",
    "                if len(sentence.split(' ')) < 30: \n",
    "                    #Verifica se a frase não está presente no dicionário de pontuações das frases.\n",
    "                    if sentence not in sentence_scores.keys(): \n",
    "                        #Atribui a pontuação da palavra à frase no dicionário de pontuações das frases.\n",
    "                        sentence_scores[sentence] = freq_dist[word] \n",
    "                    #Se a frase já estiver presente no dicionário de pontuações das frases.\n",
    "                    else:\n",
    "                        #Incrementa a pontuação da frase com a pontuação da palavra.\n",
    "                        sentence_scores[sentence] += freq_dist[word] \n",
    "    # Obtenha as frases mais importantes\n",
    "    #Ordena as frases com base em suas pontuações, do maior para o menor, e seleciona as três primeiras frases.\n",
    "    summarized_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:3]\n",
    "    #Une as frases selecionadas em um único resumo, separadas por espaço.\n",
    "    summary = ' '.join(summarized_sentences) \n",
    "    return summary\n",
    "\n",
    "# Caminho da imagem\n",
    "image_path = \"./texto.png\"\n",
    "\n",
    "# Realizar OCR\n",
    "print(\"Performing OCR...\")\n",
    "ocr_text = perform_ocr(image_path)\n",
    "print(\"OCR Result:\")\n",
    "print(ocr_text)\n",
    "print()\n",
    "\n",
    "# Realizar sumarização\n",
    "print(\"Performing Summarization...\")\n",
    "summary_text = perform_summarization(ocr_text)\n",
    "print(\"Summary:\")\n",
    "print(summary_text)\n",
    "print()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
